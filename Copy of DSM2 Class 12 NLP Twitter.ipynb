{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.6.1"},"colab":{"provenance":[{"file_id":"18lK6HOyVFo57C6h87fwQH6TXs3Qz7aW5","timestamp":1669834221721},{"file_id":"1in4HCcVnaPki9uTr9Lioc6eiaddK_fev","timestamp":1669719850273},{"file_id":"1ILBq9wY0bDfv9CFTUspFA9BBNruHtgLX","timestamp":1637088853970}],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"ge6fo7ptXqjK"},"source":["**Warning!** The following packages take several minutes to load. "]},{"cell_type":"code","metadata":{"id":"IpIZQgsP5xWI"},"source":["install.packages(c(\"tm\", \"wordcloud\", \"proxy\", \"qdap\", \"rpart.plot\", \"SnowballC\"))\n","library(tm)\n","library(wordcloud)\n","library(proxy)\n","library(stringr)\n","library(rpart)\n","library(rpart.plot)\n","library(SnowballC)\n","library(tidyverse)\n","tryCatch(library(qdap), error=function(x)\"\")\n","options(repr.matrix.max.cols=500)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"owrJPSMH5xWa"},"source":["# Twitter\n","\n","<div>\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Twitter-logo.svg/300px-Twitter-logo.svg.png\" width=\"200\"/>\n","</div>\n","\n","In this class we will use data from Twitter to explore **natural language processing (NLP)**, a subfield of computer science and linguistics that is concerned with the statistical analysis of human language data. In this notebook we will retrieve a sample of tweets from Twitter and process the language of the tweets into features that can be used in machine learning algorithms. Then we will use these features to build a model that can predict how many retweets a given tweet will receive based on the language of the tweet.\n","\n","---\n","\n","The code cell below reads in tweets based on the following keywords: **Netflix** and **Squid Game**. "]},{"cell_type":"code","metadata":{"id":"s7svB3y_6Q96"},"source":["system(\"gdown --id 1VfiKb3U9GCJ8JGali5ImCsnj6qVwImbD\")\n","tweets <- read_csv(\"tweets.csv\") %>% filter(language==\"en\")\n","head(tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nKEh8XhK5xYI"},"source":["## Regular Expressions & Data Cleaning\n","\n","To prepare our data for the predictive model, we want to clean the tweets by removing any non-language text that is not helpful in predicting the number of retweets. For example, tweets often contain hyperlinks to external webpages in the form of URLs that begin with $\\texttt{http}$. Although all hyperlinks start with a common set of characters, the characters that follow $\\texttt{http}$ are unique to each URL, so we cannot do a simple search and replace. Therefore, we need some other method to remove hyperlinks. \n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"2fwURRQc_xbV"},"source":["### Introduction to Regular Expressions\n","\n","A **regular expression (or regex)** is a sequence of characters that defines a general search pattern, as opposed to a specific search string. Imagine that we were to search all of our tweets for the sequence $\\texttt{http}$. This would find the starting position of each URL we would like to remove, but how could we actually use this information to remove the URL completely? URLs vary in length, so we could not write a general rule like \"remove $\\texttt{http}$ and the next $n$ characters\". Instead we need a more flexible rule of the form \"remove $\\texttt{http}$ and any characters that follow it *up until the next word in the text*\". This is the purpose of regular expressions. "]},{"cell_type":"markdown","metadata":{"id":"4kC1dyp-_1V3"},"source":["#### Step 1: Basic regular expressions\n","\n","To start simple, let's imagine we're searching for all instances of the pattern \"at\" within the tweet \"@AlecBaldwin hates The Cat in the Hat\". Here we show this string with each character labeled by its position:\n","\n","|@|A|l|e|c|B|a|l|d|w|i|n|&nbsp;|h|a|t|e|s|&nbsp;|T|h|e|&nbsp;|C|a|t|&nbsp;|i|n|&nbsp;|t|h|e|&nbsp;|H|a|t|\n","|-|-|-|-|-|-|-|-|-|-|-|-|------|-|-|-|-|-|------|-|-|-|------|-|-|-|------|-|-|------|-|-|-|------|-|-|-|\n","|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33|34|35|36|37|\n","\n","The $\\texttt{gregexpr}$($\\texttt{pattern, string}$) function will find the starting index of all instances of the regular expression $\\texttt{pattern}$ within $\\texttt{string}$:"]},{"cell_type":"code","metadata":{"id":"B5NAavK35xYK"},"source":["stringS1 = '@AlecBaldwin hates The Cat in the Hat'\n","gregexpr('at', stringS1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B61M1yRB5xYW"},"source":["In the above example our regular expression is not very interesting, as we are just searching for specific alphanumeric characters within the string. Imagine that instead of searching for the starting position of \"at\", we want the starting position of the entire word that contains \"at\". In our simple example, this means that our regular expression needs to match patterns of \"at\" preceded by a single letter. \n","\n","The regular expression $\\texttt{[[:alpha:]]}$ will match any single alphanumeric character, lowercase or capitalized. This means that the regular expression $\\texttt{[[:alpha:]]at}$ will match \"hat\", \"Cat\", and \"Hat\":"]},{"cell_type":"code","metadata":{"id":"1TjPjX0J5xYb"},"source":["gregexpr('[[:alpha:]]at', stringS1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZvbkRCtG5xYk"},"source":["Instead of just getting the starting indices of the matches, we can get the matches themselves by wrapping our call to $\\texttt{gregexpr}$() with $\\texttt{regmatches}$($\\texttt{string, gregexpr}$($\\texttt{pattern, string}$)):"]},{"cell_type":"code","metadata":{"id":"KiOpiAs85xYl"},"source":["regmatches(stringS1, gregexpr('[[:alpha:]]at', stringS1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CR0opzmi5xYs"},"source":["#### Step 2: Regular expressions with quantifiers and wildcards\n","\n","Now imagine the full tweet we are processing is:\n","> @AlecBaldwin hates The Cat in the Hat, see https://www.rottentomatoes.com/m/cat_in_the_hat#contentReviews and http://www.rogerebert.com/reviews/dr-seuss-the-cat-in-the-hat-2003\n","\n","Because we would like to remove the hyperlinks from the text, we need to compose a regular expression that will match both URLs at the end of the tweet.\n","\n","We can start with the characters $\\texttt{http}$, as these are common to our hyperlinks. However, some addresses follow the *secure* hypertext transfer protocol, so the $\\texttt{http}$ is followed by an $\\texttt{s}$. Other sites do not follow the secure version of the protocol, so $\\texttt{http}$ is followed directly by  $\\texttt{://www.}$. We can account for this in our regular expression by using the **quantifier** {n,m}, which means \"match between n and m repetitions of the previous character\". This means that the regular expression $\\texttt{https$\\texttt{\\{}$0,1$\\texttt{\\}}$}$ will search for \"http\" with either zero or one \"s\" on the end. "]},{"cell_type":"code","metadata":{"id":"DLwfkJUE5xYu"},"source":["stringS2 = \"@AlecBaldwin hates The Cat in the Hat, see https://www.rottentomatoes.com/m/cat_in_the_hat#contentReviews and http://www.rogerebert.com/reviews/dr-seuss-the-cat-in-the-hat-2003\"\n","regmatches(stringS2, gregexpr('https{0,1}', stringS2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aiAwMwAR5xY6"},"source":["After the $\\texttt{s}$, our URLs are the same up until the website name (ignore the fact that both websites start with $\\texttt{ro}$). This means that the beginning of our regular expression could be $\\texttt{https{0,1}://www.}$ \n","\n","There are several different ways we could match the rest of the URL. The simplest way would be to use the **wildcard** character \".\" (a period). In the context of a regular expression, a period is used to match *any possible character* (except the newline delimiter $\\texttt{\\\\n}$). Because we are not sure what will come after $\\texttt{www.}$ - it could be letters, numbers, underscores, dashes, etc. - the period can be used to match any and all of these characters.\n","\n","A single period in a regular expression will match a single character of any type. However, this is not what we want; because URLs vary in length, we want our regular expression to match an undefined number of characters of any type. We cannot use the $\\texttt{{n,m}}$ quantifier we saw before because we would have to specify $\\texttt{n}$ and $\\texttt{m}$, which are unknown. Instead we can use the \\* (an asterisk) quantifier, which matches *zero or more* of the character that comes before it. By combining . and \\*, the regular expression .\\* means \"match an arbitrarily-long sequence of any characters\". \n","\n","Note that URLs always feature an actual period after $\\texttt{www}$. Because the period character has a special meaning within regular expressions, if we want to match an actual period we need to \"escape\" its wildcard behavior with two backslashes. The regular expression $\\texttt{\\\\\\\\.}$ will match an actual period. \n","\n","Putting all of this together, our regular expression so far is $\\texttt{https{0,1}://www\\\\\\\\..*}$ Let's try applying this regular expression to our tweet:"]},{"cell_type":"code","metadata":{"id":"pAad7Yn-5xY7"},"source":["regmatches(stringS2, gregexpr(\"https{0,1}://www\\\\..*\", stringS2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aaUpaBbT5xZC"},"source":["#### Step 3: Greedy matching"]},{"cell_type":"markdown","metadata":{"id":"TKoos9k15xZD"},"source":["What happened? Instead of matching the two URLs separately, the regular expression matched both URLs and the characters in between as one big string. This is because we did not define the boundary of our wildcard sequence $\\texttt{.*}$. This sequence will match any set of characters of any length, so after $\\texttt{https://www.}$ in the first URL the expression just matched the rest of the tweet. \n","\n","To solve this we need to define the boundary of our URLs. Let's ignore the second URL for a minute and focus on the first URL, which is followed by a space. In regular expressions spaces are represented by $\\texttt{[[:space:]]}$, so the expression $\\texttt{https{0,1}://www\\\\\\\\..*[[:space:]]}$ will match $\\texttt{https://www.}$ followed by any sequence of characters *up until a space*. Let's apply this to our tweet:"]},{"cell_type":"code","metadata":{"id":"F6AxwHMH5xZF"},"source":["regmatches(stringS2, gregexpr(\"https{0,1}://www\\\\..*[[:space:]]\", stringS2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X0A85nlE5xZJ"},"source":["We are getting closer! The issue with the current expression is that it matched our first URL *plus* \" and \". Why did this happen? \n","\n","By default, the asterisk \\* quantifier is **greedy**, meaning it will match the longest string possible. This means that $\\texttt{.*[[:space:]]}$ will match any sequence of characters up until *the last space it can find*. The last space in our string is after the \"and\", so the expression matches up until that point. The first space is captured in the wildcard expression .\\*, which matches any type of character *including spaces*. \n","\n","We can turn greedy matching off by adding a question mark (?) directly after the asterisk (\\*). This means that $\\texttt{.*?[[:space:]]}$ will match any sequence of characters up until *the first space it can find*, which is what we want:"]},{"cell_type":"code","metadata":{"id":"FjV2g1YX5xZK"},"source":["regmatches(stringS2, gregexpr(\"https{0,1}://www\\\\..*?[[:space:]]\", stringS2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0S1svHHv5xZQ"},"source":["#### Step 4: Anchors and \"or\"\n","\n","Now that we've captured the first URL, we need to modify our expression so that it also captures the second one. The issue with the current expression is that it only captures URLs that end with space, but our second URL is not followed by a space because it comes at the end of the tweet. We can solve this by changing $\\texttt{[[:space:]]}$ to the **anchor** character $\\texttt{\\$}$, which matches the end of the string. This means the regular expression $\\texttt{https{0,1}://www\\\\\\\\..*?\\$}$ will match any URLs that appear at the end of the tweet. \n","\n","To capture both tweets, we want the end of the expression to match *either* a space *or* the end of the string. We can include **or statements** in our regular expression with the pipe character (|) surrounded by parentheses. The expression $\\texttt{([[:space:]]}$ | $\\texttt{\\$)}$ will match either a space or an end of string:"]},{"cell_type":"code","metadata":{"id":"tZtsjYJ95xZR"},"source":["regmatches(stringS2, gregexpr(\"https{0,1}://www\\\\..*?([[:space:]]|$)\", stringS2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qjTL_Vav5xZU"},"source":["#### Step 5: Removing matches\n","\n","Ultimately our goal is to remove the URLs from our tweets as part of the data cleaning process. Now that we have come up with a regular expression that matches URLs, we can remove them using the $\\texttt{gsub}$($\\texttt{pattern, replacement, string}$) function. This function replaces any instances of $\\texttt{pattern}$ within $\\texttt{string}$ with $\\texttt{replacement}$. If $\\texttt{pattern}$ is our regular expression for URLs and $\\texttt{replacement}$ is the empty string \"\", $\\texttt{gsub}$() will effectively remove the URLs from our tweet:"]},{"cell_type":"code","metadata":{"id":"1pfU-oix5xZV"},"source":["gsub(\"https{0,1}://www\\\\..*?([[:space:]]|$)\", \"\", stringS2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DURZjZAw5xZd"},"source":["### Clean Tweets with Regular Expressions\n","\n","Below we use the $\\texttt{gsub}$() function to remove the URLs from our tweets. Do not worry about following the logic of the regular expression - this pattern was designed to match many possible configurations of web addresses (we retrieved it from [here](https://stackoverflow.com/a/26498790))."]},{"cell_type":"code","metadata":{"id":"SVKKKkde5xZf"},"source":["urlRegex = \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n","tweets$tweet_cleaned = gsub(urlRegex, \"\", tweets$tweet)\n","tweets$tweet_cleaned = gsub('[^\\x20-\\x7E]', '', tweets$tweet_cleaned)\n","head(tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nshxF3A55xZo"},"source":["### Drop Duplicates\n","\n","Some retweets will appear several times, so we drop the duplicates by using the $\\texttt{duplicated}$() function."]},{"cell_type":"code","metadata":{"id":"QI_P0kj_5xZo"},"source":["tweets = tweets[!duplicated(tweets[,c(\"tweet\")]),]\n","head(tweets)\n","dim(tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8U1ZvfQ65xZs"},"source":["### Divide Data\n","\n","In this notebook we will separately analyze tweets with no retweets and tweets that were retweeted. Then at the end we will use both types of tweets to build a predictive model that estimates the number of retweets a given tweet will receive. Below we separate our tweet data into $\\texttt{noRetweets}$ and $\\texttt{retweets}$.   "]},{"cell_type":"code","metadata":{"id":"F9AYCeEv5xZt"},"source":["noRetweets = tweets[tweets$retweets_count == 0,]\n","dim(noRetweets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QfAmmnsv5xZ0"},"source":["retweets = tweets[tweets$retweets_count > 0,]\n","dim(retweets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oW22A3O05xZ7"},"source":["## Analyze Tweets with No Retweets\n","\n","In this section we will analyze tweets that have no retweets. In the next section, you will apply the same analyses to the tweets that do have retweets. \n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"1H8N77nC_8qp"},"source":["### Process Text Corpus <a class=\"anchor\" id=\"process-text-corpus\"></a>\n","\n","Within NLP, a **corpus** refers to a collection of texts or documents that are being analyzed. In order to analyze our corpus of tweets, we will be using the $\\texttt{tm}$ package (\"tm\" stands for text mining).  \n"]},{"cell_type":"markdown","metadata":{"id":"3qqQjpm3_80m"},"source":["#### Step 1: Create Corpus object\n","\n","The first step to analyzing text with the $\\texttt{tm}$ package is to create a $\\texttt{Corpus}$ object using $\\texttt{Corpus}$($\\texttt{VectorSource}$()). By passing all of our tweets into this function, the $\\texttt{tm}$ package simply organizes them into a format that makes them easier to process. This step does not manipulate or analyze our tweets in any way - it just stores the tweets in an object that the $\\texttt{tm}$ package is designed to work with. "]},{"cell_type":"code","metadata":{"id":"U7MdZh_h5xZ8"},"source":["noRetweetCorpus = Corpus(VectorSource(noRetweets$tweet_cleaned))\n","noRetweetCorpus"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVok2E5x5xZ_"},"source":["The output of $\\texttt{noRetweetCorpus}$ indicates how many documents (or tweets) our corpus has; this should equal the number of rows of $\\texttt{noRetweets}$:"]},{"cell_type":"code","metadata":{"id":"qeXX24Z25xaB"},"source":["nrow(noRetweets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2CrmXyt5xaF"},"source":["#### Step 2: Create term document matrix <a class=\"anchor\" id=\"term-document-matrix\"></a>\n","\n","Up until now we have ignored the question of how exactly machine learning algorithms use language data to make predictions. After all, these algorithms are only able to manipulate numeric data, not strings. Therefore, we need some method of converting our texts into numbers. \n","\n","The simplest method for converting a corpus into a numeric format is to simply count the number of times each word in the corpus appears in each document. We start this process by collecting the set of all words used in the corpus. In our example, this means we collect every unique word used across all of the tweets we are analyzing. Then for each document (or tweet) we count the number of times each of those words occurs. Because a given tweet only contains a small fraction of the words used in the entire corpus, most of the entries for each document will be zero.\n","\n","One way to represent this data is with a **term-document matrix**. The rows of this matrix represent each word in the corpus, and the columns represent each document. Each entry ($i$, $j$) represents the number of times word $i$ appears in document $j$. We can easily create this matrix with the $\\texttt{tm}$ package by applying the $\\texttt{TermDocumentMatrix}$() function to our corpus:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"FNShvyFM5xaF"},"source":["noRetweetTDM = TermDocumentMatrix(noRetweetCorpus)\n","as.matrix(noRetweetTDM)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YRgBOeIz5xaK"},"source":["Note that his method of representing our documents completely ignores the syntax and grammatical structures of our text. The only information it captures is the number of times each word appears in a given document. This type of representation is known as a **bag-of-words model**. \n","\n","#### Step 3: Cleaning our term document matrix\n","\n","In order to improve the quality of our analyses, there is some further text cleaning we can do. \n","\n","##### Remove stop words \n","\n","Within NLP, **stop words** are any words that we want to filter out of our text data before conducting our analyses. For example, consider the word \"and\", which likely appears in the term document matrix from the previous step. Although \"and\" plays an important grammatical role in the English language, it is more functional than it is semantically substantive. Because we are using a bag-of-words model in which grammar is effectively ignored, \"and\" is not meaningful for the type of analyses we are conducting. Therefore, we will consider \"and\" a stop word and remove it from our tweets. \n","\n","Although there is not one agreed upon list of stop words, for our purposes we will use the default list provided by R. We can access this list with $\\texttt{stopwords(\"english\")}$:"]},{"cell_type":"code","metadata":{"id":"Xt8SWsPC5xaK"},"source":["stopwords(\"english\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lsBW1J4f5xaO"},"source":["##### Word stemming \n","\n","Another important pre-processing that is common in NLP is **stemming**, which attempts to reduce words to their root forms. \n","\n","For example, consider the words \"finance\", \"financial\", and \"financing\". The affixes on these words are used to mark their parts-of-speech and indicate their roles within sentences. \"finance\" is a noun referring to investment management; \"financial\" is an adjective that marks something as being related to finance; \"financing\" is a present participle or gerund that refers to the act of providing money. \n","\n","These differences are important syntactically, but our bag-of-words approach is more concerned with semantics than syntax. Therefore, we can stem these words into a common root (*e.g.* \"financ\") so that they are recognized as referring to the same concept despite their syntactic differences. As you will see below, the $\\texttt{tm}$ package offers a pre-built stemmer that is designed to remove affixes from words, reducing them to their root forms."]},{"cell_type":"markdown","metadata":{"id":"jeic2PErAGFn"},"source":["##### Miscellaneous preprocessing\n","\n","Finally, we also want to:\n","+ Remove punctuation and numbers from our tweets, as these are not relevant for our bag-of-words model.\n","+ Convert all of our words to lower case so that capitalization does not affect our analyses.\n","    * For example, \"coronavirus\" and \"Coronavirus\" should be treated as the same word."]},{"cell_type":"markdown","metadata":{"id":"Wn-KikjFAGNV"},"source":["##### Apply cleaning steps\n","\n","Fortunately, the $\\texttt{TermDocumentMatrix}$() function from the $\\texttt{tm}$ package has a $\\texttt{control}$ parameter that allows us to apply all of these cleaning steps automatically. First we create a list called $\\texttt{processingSettings}$ with the following parameters, which are applied when the term document matrix is created:\n","+ $\\texttt{stopwords = stopwords(\"english\")}$\n","+ $\\texttt{stemming = TRUE}$\n","+ $\\texttt{removePunctuation = TRUE}$\n","+ $\\texttt{removeNumbers = TRUE}$\n","+ $\\texttt{tolower = TRUE}$"]},{"cell_type":"code","metadata":{"id":"o8RL640M5xaP"},"source":["processingSettings = list(stopwords = stopwords(\"english\"), stemming=TRUE, \n","                          removePunctuation = TRUE, removeNumbers = TRUE, \n","                          tolower = TRUE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oTc5Zoff5xaX"},"source":["Now we apply the $\\texttt{TermDocumentMatrix}$() function as before, but this time set the $\\texttt{control}$ parameter equal to our $\\texttt{processingSettings}$ list:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"i6NuKDMn5xaY"},"source":["noRetweetTDMClean = TermDocumentMatrix(noRetweetCorpus, control=processingSettings)\n","as.matrix(noRetweetTDMClean)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mBIC6vxD5xae"},"source":["Compare this term document matrix to the one from the previous step, which did not include any preprocessing. \n","\n","### Create Word Cloud <a class=\"anchor\" id=\"create-word-cloud\"></a>\n","\n","Using the term document matrix we created in the last section, we can create a **word cloud** that visualizes the most common words in our corpus. First we apply the $\\texttt{rowSums}$() function to our term document matrix to get the count of each word in the corpus across all of the documents:"]},{"cell_type":"code","metadata":{"id":"dpIGhqFp5xaf"},"source":["noRetweetTokenCounts = rowSums(as.matrix(noRetweetTDMClean))\n","noRetweetTokenCounts"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZN_1QPl5xao"},"source":["Then we just need to pass $\\texttt{names(noRetweetTokenCounts)}$ and $\\texttt{noRetweetTokenCounts}$ into the $\\texttt{wordcloud}$() function from the $\\texttt{wordcloud}$ package. The $\\texttt{min.freq}$ parameter is used to determine how many words from our corpus we want included in the word cloud:"]},{"cell_type":"code","metadata":{"id":"kSKeuvhc5xao"},"source":["wordcloud(names(noRetweetTokenCounts), noRetweetTokenCounts, min.freq = 20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7HSJ0o7V5xar"},"source":["## Analyze Tweets with Retweets\n","\n","In this section, you will repeat the analyses from the previous section on the set of tweets that were retweeted. \n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"VfknMKFVAQAk"},"source":["### Process Text Corpus\n","\n","First, create a processed corpus of tweets that were retweeted. "]},{"cell_type":"code","metadata":{"id":"2PNplXxk5xas"},"source":["retweetCorpus = Corpus(VectorSource(retweets$tweet_cleaned))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cnGLRxx95xaw"},"source":["Then use this corpus to create a term document matrix as we did before:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TrYDHyTY5xax"},"source":["retweetTDM = TermDocumentMatrix(retweetCorpus)\n","as.matrix(retweetTDM)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wlfCkb7L5xa6"},"source":["### Create Word Cloud\n","\n","Finally, use the term document matrix from the previous step to create a word cloud. Are there any differences in the frequently used words in the non-retweeted and retweeted tweets?"]},{"cell_type":"code","metadata":{"id":"Q-PXwerY5xa6"},"source":["retweetTokenCounts = rowSums(as.matrix(retweetTDM))\n","wordcloud(names(retweetTokenCounts), retweetTokenCounts, min.freq = 20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DI2JR1HH5xbA"},"source":["## Build Predictive Model\n","\n","Now we will use what we have learned to build a machine learning model that predicts how many retweets a given tweet will receive. Here we will return to the full data set with all of the tweets we collected ($\\texttt{tweets}$). \n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"h38zCs8SAT-V"},"source":["### Import Sentiment Data\n","\n","One common task within NLP is known as **sentiment analysis**. The goal of sentiment analysis is to use natural language processing to analyze the tone and attitude of a text. This is often used in business settings to analyze online customer reviews and open-ended responses from product surveys. \n","\n","We can use sentiment analysis to help build our predictive model by creating a \"happiness\" feature that measures the positivity of each tweet. The Computational Story Lab at the University of Vermont offers a corpus of English words and their associated happiness scores as part of its Hedonometer project (*see* [here](http://hedonometer.org/about.html)). As explained on their website:\n","\n","> To quantify the happiness of the atoms of language, we merged the 5,000 most frequent words from a collection of four corpora: Google Books, New York Times articles, Music Lyrics, and Twitter messages, resulting in a composite set of roughly 10,000 unique words. Using Amazonâ€™s Mechanical Turk service, we had each of these words scored on a nine point scale of happiness: (1) sad to (9) happy. You can explore the average scores of each word on our words page, or download the entire list from the publication supplement [here](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752).\n","\n","We can import this data set from the $\\texttt{qdap}$ package by running $\\texttt{data(\"labMT\")}$. Each row represents a unique word in their corpus, and $\\texttt{happiness}$\\_$\\texttt{average}$ is the average happiness score of each word from the survey."]},{"cell_type":"code","metadata":{"id":"7TJ9iM4I5xbA"},"source":["data(\"labMT\")\n","head(labMT)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LyVFTnzx5xbG"},"source":["For a given tweet, we can create a happiness score by taking the average of $\\texttt{happiness}$\\_$\\texttt{average}$ for each word that appears in the tweet. Recall that as part of our pre-processing we stem the words in our text data. To ensure that the words in our tweets match up to the words in Hedonometer's happiness corpus, we need to stem the words in $\\texttt{labMT}$ using the $\\texttt{stemDocument}$() function:"]},{"cell_type":"code","metadata":{"id":"ugPDAFIR5xbG"},"source":["labMT$word = stemDocument(labMT$word)\n","head(labMT)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"43Qt4SQI5xbK"},"source":["Note that after stemming the $\\texttt{word}$ column there are now duplicate entries in $\\texttt{labMT}$ (for example, $\\texttt{happiness}$ and $\\texttt{happy}$ have both been stemmed to $\\texttt{happi}$). To correct this, we need to collapse duplicate rows together and take the average value of all the columns over those duplicate rows. We can do this using the $\\texttt{aggregate}$() function:"]},{"cell_type":"code","metadata":{"id":"xmVmkzGu5xbK"},"source":["labMT = aggregate(. ~ word, data=labMT, FUN=mean)\n","labMT = labMT[order(labMT$happiness_rank),]\n","head(labMT)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1bLqWS0Z5xbP"},"source":["### Document Term Matrix\n","\n","In the previous section we worked with term-document matrices in which the rows represented the words in our corpus and the columns represented the documents. As you might suspect, a **document-term matrix** is simply the inverse of a term-document matrix; the rows represent the documents and the columns represent the words in the corpus. \n","\n","As with the term-document matrix, we can easily create a document-term matrix using the $\\texttt{DocumentTermMatrix}$() function from $\\texttt{tm}$. Note that we pass in the same $\\texttt{control}$ parameter as before, so we are applying all of the same pre-processing steps. We convert the document-term matrix to a data frame so it is easier to work with. "]},{"cell_type":"code","metadata":{"id":"SfgFyNTj5xbP"},"source":["# Create corpus from all tweets\n","tweetCorpus = Corpus(VectorSource(tweets$tweet_cleaned))\n","\n","# Create document term matrix based on corpus\n","tweetDTM = DocumentTermMatrix(tweetCorpus, control=processingSettings)\n","\n","# Convert to data frame\n","tweetDTM = as.data.frame(as.matrix(tweetDTM))\n","head(tweetDTM)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D10_311q5xbV"},"source":["### Feature Engineering\n","\n","Now we will create some features that will be used in our predictive model. "]},{"cell_type":"markdown","metadata":{"id":"WdBFLuL8AXmI"},"source":["#### Step 1: Calculate total happiness score for each tweet\n","\n","In this step we will calculate the total happiness score for each document in our corpus using the happiness scores from the Hedonometer data. The function we define below accepts a document-term matrix and calculates the total happiness score for each row (or document) in that matrix. \n","\n"]},{"cell_type":"code","metadata":{"id":"YGKr4Om-5xbY"},"source":["calculate_happiness = function(DTM){\n","    DTMCount <- DTM\n","    # For each word in the corpus (i.e. each column in the document-term matrix):\n","    for (col in names(DTM)){\n","        \n","        # 1. Calculate that word's happiness score according to Hedonometer (or 0 if word is missing from Hedonometer)\n","        wordHappinessScore = max(0, labMT[labMT$word==col,]$happiness_average)\n","        \n","        # 2. Multiply the word's happiness score by the count of that word in each document\n","        DTM[,col] = wordHappinessScore * DTM[col]\n","    }\n","    \n","    # For each tweet, average the happiness scores for all the words\n","    DTM$happiness_total = rowSums(DTM) / rowSums(DTMCount)\n","    \n","    # Return the total happiness scores for each tweet\n","    return(DTM$happiness_total)\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SEJJA_4r5xbd"},"source":["We can then use this function to create a column with the happiness score for each tweet:"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"0HR7czH45xbd"},"source":["tweets$happiness_score <- calculate_happiness(tweetDTM)\n","head(tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eA5Jds0K5xbg"},"source":["#### Step 2: Handle count for each tweet\n","\n","Next we create a feature called $\\texttt{handle}$\\_$\\texttt{count}$ that contains the number of user handles included in each tweet. We can count the number of times that \"@\" appears in each tweet with the $\\texttt{str}$\\_$\\texttt{count}$() function from the $\\texttt{stringr}$ package:"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"uae-fyht5xbh"},"source":["tweets$handle_count = str_count(tweets$tweet_cleaned, \"@\")\n","head(tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hUpHP5qg5xbp"},"source":["### Build Model\n","\n","Now that we have cleaned our tweets and done some feature engineering, we will fit a regression tree that predicts number of retweets. By applying $\\texttt{summary}$() to our model we can see the importance of each feature (under \"Variable importance\")."]},{"cell_type":"code","metadata":{"id":"CnfAaSsW5xbq"},"source":["model = rpart(retweets_count ~ happiness_score + handle_count,\n","                           data=tweets)\n","rpart.plot(model, branch=0.3, tweak=1.5)"],"execution_count":null,"outputs":[]}]}